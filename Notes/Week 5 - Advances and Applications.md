## Conditional latent variable models 

- How to construct conditional LVM so that we can specify conditions in the generative process
![[Screenshot 2023-02-06 at 19.11.16.png]]
- ![[Screenshot 2023-02-06 at 19.11.40.png]]
- Very parameter inefficient and cannot handle continuous variables ![[Screenshot 2023-02-06 at 19.12.12.png]]
- We only need to train one network 
- latent variable z does not represent the feature 

How to train?

![[Screenshot 2023-02-06 at 19.13.05.png]]
- Again intractable since $z$ is latent (unobserved)
- ![[Screenshot 2023-02-06 at 19.13.38.png]]
- ![[Screenshot 2023-02-06 at 19.14.07.png]]

## Conditional GANS

![[Screenshot 2023-02-06 at 19.14.59.png]]
- We need to keep track the labels with each conditional distribution 

## Generative Model Architecture Design 

DCGAN - Used full convolutional architecture 
Tricks used in the DCGAN architecture & training: 
- Replace pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator). 
- Use batchnorm. 
- Remove fully connected hidden layers for deeper architectures. 
- Use LeakyReLU activation in the discriminator for all layers.

LAPGAN - Generator’s multi-scale architecture
- Start generation for low-resolution images
- Generate higher-resolution images conditioned on the lower-resolution ones![[Screenshot 2023-02-06 at 19.19.52.png]]
- Multiple discriminators in use, paired with generators at different resolutions
![[Screenshot 2023-02-06 at 19.20.46.png]]
- At each resolution, check whether the “fine details” generated by $G_i$, matches the real ones. ![[Screenshot 2023-02-06 at 19.21.37.png]]
- We simply downscale or upscale the original image when training 

## Progressive GAN
Progressively building GAN generator and discriminator:
- High-res images downscaled to get training data of low resolutions 
- Train a GAN starting from 4x4 images 
- Add new layers into generator and discriminator 
- Adapt old & new layers by GAN training with 8x8 
- Continue with 16x16, 32x32…![[Screenshot 2023-02-06 at 19.26.18.png]]
- Idea, low resolution images are easier to train the models with 

## StyleGAN
Disentangling different sources of randomness: 
- Latent variable z is transformed to “style” representation 
- This “style” w controls generation at every resolutions 
- Fine details generated with noise at different scales![[Screenshot 2023-02-06 at 19.27.30.png]]
- ![[Screenshot 2023-02-06 at 19.27.42.png]]

## NVAE - improved VAE image generation

State-of-the-art VAE for image generation (2020): 
- Hierarchical LVM with multi-scale architecture 
- Using residual networks for the r blocks 
- BatchNorm in usage![[Screenshot 2023-02-06 at 19.29.37.png]]

## Progress in architecture design 
- GAN progression:
	- DCGAN – fully convolutional neural networks
	- LAPGAN & Progressive GAN – multi-scale architectures 
	- StyleGAN – disentangling sources of randomness 
- VAE progression: 
	- Hierarchical LVMs • Tuning the KL regulariser
	- Deep learning tricks applied 
	- Incorporate design ideas from GAN networks

## Combining VAEs and GANs 

![[Screenshot 2023-02-06 at 19.31.25.png]]
Adversarial auto-encoders: 
- Reconstruction loss in x space (similar to VAEs) 
- Adversarial loss in z space (similar to GANs)

## Applications of Generative models

- Super resolution - scale up images to improve resolution 
- Image-to-Image translation![[Screenshot 2023-02-06 at 19.34.13.png]]
- Other types of generative models 
	- (discrete-time) normalising flow
	- Energy-based models
	- Score based generative models
	- Denoising diffusion probabilistic models

## Model design in practice

- “Algorithms/paradigms” vs “network architecture” 
	- VAE/GAN/flow/EBM/SGM as “algorithms/modelling paradigms” 
	- MLP/CNN/Transformer as “network architecture”, construct components of our model 
- When should we use VAE or GANs, no clear answer depends on the application
	- GAN is often preferred for better visual quality (VAEs & others are catching up) 
	- VAE, flow, etc. preferred for applications that need good likelihood estimates • E.g. neural data compression 
	- Practical solutions are often a mix of many paradigms

Concerns emerge as generative models improve: 
- Generated visuals are getting very photo-realistic 
- Has been used in fraud and scam videos 
- Regulations and detection techniques needed